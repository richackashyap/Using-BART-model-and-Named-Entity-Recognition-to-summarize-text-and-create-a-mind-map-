# -*- coding: utf-8 -*-
"""MindMapGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CC0UAYK0e9IIQNssAL6RdF9gDGuG06J9
"""

!pip install pytextrank

import spacy
from transformers import pipeline
import graphviz
from IPython.display import display, Image
import time
from collections import Counter

# Load spaCy model for NLP
nlp = spacy.load('en_core_web_sm')

# Load summarization pipeline from Hugging Face
summarizer = pipeline('summarization', model="facebook/bart-large-cnn")

def summarize_text(text):
    input_length = len(text.split())
    max_length = min(input_length // 2, 200)  # Ensuring it doesn't exceed 200 tokens
    min_length = min(input_length // 4, 100)  # Ensuring it doesn't exceed 100 tokens

    try:
        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        print(f"Error summarizing text: {e}")
        return text  # Return the original text if summarization fails

def extract_sentences(summary):
    doc = nlp(summary)
    sentences = [sent.text for sent in doc.sents]
    return sentences

def extract_main_topic(text):
    # Extract named entities and their frequencies
    doc = nlp(text)
    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LOC', 'PRODUCT']]
    entity_freq = Counter(entities)

    # Find the most frequent entity
    if entity_freq:
        main_topic = entity_freq.most_common(1)[0][0]
    else:
        # Fallback: Use the most significant sentence based on keyword frequency
        sentences = [sent.text for sent in doc.sents]
        words = [token.text for token in doc if token.is_alpha and not token.is_stop]
        word_freq = Counter(words)

        sentence_scores = []
        for sentence in sentences:
            sentence_doc = nlp(sentence)
            score = sum(word_freq[token.text] for token in sentence_doc if token.text in word_freq)
            sentence_scores.append((score, sentence))

        if sentence_scores:
            main_topic = max(sentence_scores, key=lambda x: x[0])[1]
        else:
            main_topic = text.split('.')[0]  # Fallback to the first sentence if all else fails

    return main_topic

def create_mind_map(main_topic, sentences, filename='mind_map'):
    dot = graphviz.Digraph(comment='Mind Map')

    # Add main topic
    dot.node('0', main_topic)

    # Add related points
    for i, sentence in enumerate(sentences, 1):
        dot.node(str(i), sentence)
        dot.edge('0', str(i))

    # Save the mind map to a file
    dot.render(filename, format='png', cleanup=False)

    return filename + '.png'

def main():
    while True:
        text = input("Enter a paragraph: ")

        # Step 1: Extract main topic
        main_topic = extract_main_topic(text)
        print("\nMain Topic:")
        print(main_topic)

        # Step 2: Summarize the text
        summary = summarize_text(text)
        print("\nSummary:")
        print(summary)

        # Step 3: Extract key points
        points = extract_sentences(summary)
        print("\nKey Points:")
        for point in points:
            print("-", point)

        # Step 4: Create and display mind map
        mind_map_file = create_mind_map(main_topic, points)
        display(Image(filename=mind_map_file))

        # Slight delay before asking the user to continue
        time.sleep(1)

        # Ask the user if they want to continue
        continue_choice = input("Do you want to continue? (yes/no): ").strip().lower()
        if continue_choice != 'yes':
            break

if __name__ == "__main__":
    main()